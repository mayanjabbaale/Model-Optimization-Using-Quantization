{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxh4oLKI+dv+xJTxJ0L+YX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayanjabbaale/Model-Optimization-Using-Quantization/blob/main/POST_TRAINING_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr1JQQOwukhN",
        "outputId": "cc125885-e448-4532-e683-561a0a34c036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version --> 2.9.0+cu126\n",
            "Working on --> cuda\n",
            "Skip CPU evaluations --> False\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "from packaging import version\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.quantization import quantize_dynamic\n",
        "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
        "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\".*TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*erase_node(.*) on an already erased node.*\")\n",
        "\n",
        "print(f\"Using PyTorch version --> {torch.__version__}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Working on --> {device}')\n",
        "\n",
        "skip_cpu = False\n",
        "print(f'Skip CPU evaluations --> {skip_cpu}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), ((0.5,)))\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(\"./train\", train=True, transform=transform, download=True)\n",
        "test_data = datasets.CIFAR10(\"./eval\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              batch_size=128,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                              batch_size=128,\n",
        "                              shuffle=True,\n",
        "                              num_workers=2,\n",
        "                              drop_last=True)\n",
        "\n",
        "calibration_dataset = Subset(train_data, range(256))\n",
        "calibration_loader = DataLoader(calibration_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "6Zah8XSK8WG7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18_for_CIFAR10():\n",
        "  model = models.resnet18(weights=None, num_classes=10)\n",
        "  model.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "  model.maxpool = nn.Identity()\n",
        "\n",
        "  return model.to(device)\n",
        "\n",
        "model_to_quantize = resnet18_for_CIFAR10()"
      ],
      "metadata": {
        "id": "7PLU6Q3zAiVD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, epochs, lr=0.01, save_path=\"model.pth\", silent=False):\n",
        "  try:\n",
        "    model.train()\n",
        "  except NotImplementedError:\n",
        "    torch.ao.quantization.move_exported_model_to_train(model)\n",
        "\n",
        "  if os.path.exists(save_path):\n",
        "    if not silent:\n",
        "      print(f'Model already trained. Loading from --> {save_path}')\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    return\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for X, y in loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      logits = model(X)\n",
        "      loss = criterion(logits, y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if not silent:\n",
        "      print(f'Epoch {epoch+1} Loss={loss:.4f}')\n",
        "      evaluate(model, f'Epoch {epoch+1}')\n",
        "\n",
        "      try:\n",
        "        model.train()\n",
        "      except NotImplementedError:\n",
        "        torch.ao.quantization.move_exported_model_to_train(model)\n",
        "\n",
        "  if save_path:\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    if not silent:\n",
        "      print(f'Training complete, model saved to --> {save_path}')\n",
        "\n",
        "def evaluate(model, tag):\n",
        "  try:\n",
        "    model.eval()\n",
        "  except NotImplementedError:\n",
        "    model = torch.ao.quantization.move_exported_model_to_eval(model)\n",
        "\n",
        "  model.to(device)\n",
        "  correct = total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_dataloader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      preds = model(x).argmax(1)\n",
        "      correct += (preds == y).sum().item()\n",
        "      total += y.size(0)\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy ({tag}): {(accuracy*100):.2f}%')"
      ],
      "metadata": {
        "id": "0yrXjEaPBsCB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer:\n",
        "    \"\"\"\n",
        "    A simple timer utility for measuring elapsed time in milliseconds.\n",
        "\n",
        "    Supports both GPU and CPU timing:\n",
        "    - If CUDA is available, uses torch.cuda.Event for accurate GPU timing.\n",
        "    - Otherwise, falls back to wall-clock CPU timing via time.time().\n",
        "\n",
        "    Methods:\n",
        "        start(): Start the timer.\n",
        "        stop(): Stop the timer and return the elapsed time in milliseconds.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        if self.use_cuda:\n",
        "            self.starter = torch.cuda.Event(enable_timing=True)\n",
        "            self.ender = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    def start(self):\n",
        "        if self.use_cuda:\n",
        "            self.starter.record()\n",
        "        else:\n",
        "            self.start_time = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.use_cuda:\n",
        "            self.ender.record()\n",
        "            torch.cuda.synchronize()\n",
        "            return self.starter.elapsed_time(self.ender)  # ms\n",
        "        else:\n",
        "            return (time.time() - self.start_time) * 1000  # ms\n",
        "\n",
        "def estimate_latency(model, example_inputs, repetitions=50):\n",
        "    \"\"\"\n",
        "    Returns avg and std inference latency (ms) over given runs.\n",
        "    \"\"\"\n",
        "\n",
        "    timer = Timer()\n",
        "    timings = np.zeros((repetitions, 1))\n",
        "\n",
        "    # warm-up\n",
        "    for _ in range(5):\n",
        "        _ = model(example_inputs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for rep in range(repetitions):\n",
        "            timer.start()\n",
        "            _ = model(example_inputs)\n",
        "            elapsed = timer.stop()\n",
        "            timings[rep] = elapsed\n",
        "\n",
        "    return np.mean(timings), np.std(timings)\n",
        "\n",
        "def estimate_latency_full(model, tag, skip_cpu):\n",
        "    \"\"\"\n",
        "    Prints model latency on GPU and (optionally) CPU.\n",
        "    \"\"\"\n",
        "\n",
        "    # estimate latency on CPU\n",
        "    if not skip_cpu:\n",
        "        example_input = torch.rand(128, 3, 32, 32).cpu()\n",
        "        model.cpu()\n",
        "        latency_mu, latency_std = estimate_latency(model, example_input)\n",
        "        print(f\"Latency ({tag}, on CPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")\n",
        "\n",
        "    # estimate latency on GPU\n",
        "    example_input = torch.rand(128, 3, 32, 32).cuda()\n",
        "    model.cuda()\n",
        "    latency_mu, latency_std = estimate_latency(model, example_input)\n",
        "    print(f\"Latency ({tag}, on GPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")\n",
        "\n",
        "def print_size_of_model(model, tag=\"\"):\n",
        "    \"\"\"\n",
        "    Prints model size (MB).\n",
        "    \"\"\"\n",
        "\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size_mb_full = os.path.getsize(\"temp.p\") / 1e6\n",
        "    print(f\"Size ({tag}): {size_mb_full:.2f} MB\")\n",
        "    os.remove(\"temp.p\")"
      ],
      "metadata": {
        "id": "oHCFHsCgIDCf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_to_quantize, train_dataloader, epochs=15, save_path=\"full_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3sa6vzxI24n",
        "outputId": "bb343018-4100-4329-f8d1-502ae597ba4c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model already trained. Loading from --> full_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get full model size\n",
        "print_size_of_model(model_to_quantize, \"full\")\n",
        "\n",
        "# evaluate full accuracy\n",
        "accuracy_full = evaluate(model_to_quantize, 'full')\n",
        "\n",
        "# estimate full model latency\n",
        "estimate_latency_full(model_to_quantize, 'full', skip_cpu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKksydvIJXG9",
        "outputId": "c3c63473-b81f-4bc6-eeac-2dc23b38ba52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (full): 44.77 MB\n",
            "Accuracy (full): 80.28%\n",
            "Latency (full, on CPU): 1943.15 ± 693.62 ms\n",
            "Latency (full, on GPU): 27.41 ± 0.38 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.ao.quantization.quantize_pt2e import (\n",
        "    prepare_pt2e,\n",
        "    convert_pt2e\n",
        ")\n",
        "\n",
        "import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq\n",
        "from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer\n",
        "\n",
        "example_inputs = (torch.randn((128, 3, 32, 32)).to(device), )\n",
        "\n",
        "if version.parse(torch.__version__) >= version.parse(\"2.5\"):\n",
        "  exported_model = torch.export.export(model_to_quantize, example_inputs).module()\n",
        "else:\n",
        "  from torch._export import capture_pre_autograd_graph\n",
        "  exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs)\n",
        "\n",
        "quantizer = X86InductorQuantizer()\n",
        "quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())\n",
        "\n",
        "prepared_model = prepare_pt2e(exported_model, quantizer)\n",
        "\n",
        "def calibrate(model, loader):\n",
        "  torch.ao.quantization.move_exported_model_to_eval(model)\n",
        "  with torch.no_grad():\n",
        "    for images, labels in loader:\n",
        "      model(images.to(device))\n",
        "\n",
        "calibrate(prepared_model, calibration_loader)\n",
        "\n",
        "quantized_model = convert_pt2e(prepared_model)\n",
        "\n",
        "if version.parse(torch.__version__) >= version.parse(\"2.5\"):\n",
        "    quantized_model = torch.export.export(quantized_model, example_inputs).module()\n",
        "else:\n",
        "    quantized_model = capture_pre_autograd_graph(quantized_model, example_inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC-GrpE7USUW",
        "outputId": "9775505b-ca4d-4179-978b-962bfbe810bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3040608684.py:20: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared_model = prepare_pt2e(exported_model, quantizer)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/tmp/ipython-input-3040608684.py:30: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = convert_pt2e(prepared_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get quantized model size\n",
        "print_size_of_model(quantized_model, \"quantized\")\n",
        "\n",
        "# evaluate quantized accuracy\n",
        "accuracy_full = evaluate(quantized_model, 'quantized')\n",
        "\n",
        "# estimate quantized model latency\n",
        "estimate_latency_full(quantized_model, 'quantized', skip_cpu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZC-CttQaSyW",
        "outputId": "ecd822ed-a2e5-4b71-eef6-16be1330eb62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (quantized): 44.69 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
            "  aten_pattern = torch.export.export_for_training(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (quantized): 80.29%\n",
            "Latency (quantized, on CPU): 1585.81 ± 270.89 ms\n",
            "Latency (quantized, on GPU): 27.40 ± 1.30 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# enable the use of the C++ wrapper for TorchInductor which reduces Python overhead\n",
        "import torch._inductor.config as config\n",
        "config.cpp_wrapper = True\n",
        "\n",
        "# compiles quantized model to generate optimized model\n",
        "with torch.no_grad():\n",
        "    optimized_model = torch.compile(quantized_model)\n",
        "\n",
        "# get optimized model size\n",
        "print_size_of_model(optimized_model, \"optimized\")\n",
        "\n",
        "# evaluate optimized accuracy\n",
        "accuracy_full = evaluate(optimized_model, 'optimized')\n",
        "\n",
        "# estimate optimized model latency\n",
        "estimate_latency_full(optimized_model, 'optimized', skip_cpu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVgi2i_pcSHK",
        "outputId": "cecd3187-56f4-4707-8fd3-33ca1a937435"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (optimized): 44.69 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "W0204 11:41:06.963000 19922 torch/_inductor/utils.py:1558] [24/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (optimized): 80.30%\n",
            "Latency (optimized, on CPU): 1541.44 ± 656.25 ms\n",
            "Latency (optimized, on GPU): 53.40 ± 0.43 ms\n"
          ]
        }
      ]
    }
  ]
}