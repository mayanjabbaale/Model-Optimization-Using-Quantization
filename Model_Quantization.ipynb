{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5OuBel18ePibsPnIyt7Lk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayanjabbaale/Model-Optimization-Using-Quantization/blob/main/Model_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr1JQQOwukhN",
        "outputId": "53d95b55-677a-44d7-e93b-bf6852e2b541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version --> 2.9.0+cu126\n",
            "Working on --> cuda\n",
            "Skip CPU evaluations --> False\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "from packaging import version\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.quantization import quantize_dynamic\n",
        "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
        "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\".*TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*erase_node(.*) on an already erased node.*\")\n",
        "\n",
        "print(f\"Using PyTorch version --> {torch.__version__}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Working on --> {device}')\n",
        "\n",
        "skip_cpu = False\n",
        "print(f'Skip CPU evaluations --> {skip_cpu}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), ((0.5,)))\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(\"./train\", train=True, transform=transform, download=True)\n",
        "test_data = datasets.CIFAR10(\"./eval\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              batch_size=128,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                              batch_size=128,\n",
        "                              shuffle=True,\n",
        "                              num_workers=2,\n",
        "                              drop_last=True)\n",
        "\n",
        "calibration_dataset = Subset(train_data, range(256))\n",
        "calibration_loader = DataLoader(calibration_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zah8XSK8WG7",
        "outputId": "ef884d51-f491-4f75-a3e6-d2e696f530b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 28.6MB/s]\n",
            "100%|██████████| 170M/170M [00:08<00:00, 20.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18_for_CIFAR10():\n",
        "  model = models.resnet18(weights=None, num_classes=10)\n",
        "  model.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "  model.maxpool = nn.Identity()\n",
        "\n",
        "  return model.to(device)\n",
        "\n",
        "model_to_quantize = resnet18_for_CIFAR10()"
      ],
      "metadata": {
        "id": "7PLU6Q3zAiVD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, epochs, lr=0.01, save_path=\"model.pth\", silent=False):\n",
        "  try:\n",
        "    model.train()\n",
        "  except NotImplementedError:\n",
        "    torch.ao.quantization.move_exported_model_to_train(model)\n",
        "\n",
        "  if os.path.exists(save_path):\n",
        "    if not silent:\n",
        "      print(f'Model already trained. Loading from --> {save_path}')\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    return\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for X, y in loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      logits = model(X)\n",
        "      loss = criterion(logits, y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if not silent:\n",
        "      print(f'Epoch {epoch+1} Loss={loss:.4f}')\n",
        "      evaluate(model, f'Epoch {epoch+1}')\n",
        "\n",
        "      try:\n",
        "        model.train()\n",
        "      except NotImplementedError:\n",
        "        torch.ao.quantization.move_exported_model_to_train(model)\n",
        "\n",
        "  if save_path:\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    if not silent:\n",
        "      print(f'Training complete, model saved to --> {save_path}')\n",
        "\n",
        "def evaluate(model, tag):\n",
        "  try:\n",
        "    model.eval()\n",
        "  except NotImplementedError:\n",
        "    model = torch.ao.quantization.move_exported_model_to_eval(model)\n",
        "\n",
        "  model.to(device)\n",
        "  correct = total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_dataloader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      preds = model(x).argmax(1)\n",
        "      correct += (preds == y).sum().item()\n",
        "      total += y.size(0)\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy ({tag}): {(accuracy*100):.2f}%')"
      ],
      "metadata": {
        "id": "0yrXjEaPBsCB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer:\n",
        "    \"\"\"\n",
        "    A simple timer utility for measuring elapsed time in milliseconds.\n",
        "\n",
        "    Supports both GPU and CPU timing:\n",
        "    - If CUDA is available, uses torch.cuda.Event for accurate GPU timing.\n",
        "    - Otherwise, falls back to wall-clock CPU timing via time.time().\n",
        "\n",
        "    Methods:\n",
        "        start(): Start the timer.\n",
        "        stop(): Stop the timer and return the elapsed time in milliseconds.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        if self.use_cuda:\n",
        "            self.starter = torch.cuda.Event(enable_timing=True)\n",
        "            self.ender = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    def start(self):\n",
        "        if self.use_cuda:\n",
        "            self.starter.record()\n",
        "        else:\n",
        "            self.start_time = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.use_cuda:\n",
        "            self.ender.record()\n",
        "            torch.cuda.synchronize()\n",
        "            return self.starter.elapsed_time(self.ender)  # ms\n",
        "        else:\n",
        "            return (time.time() - self.start_time) * 1000  # ms\n",
        "\n",
        "def estimate_latency(model, example_inputs, repetitions=50):\n",
        "    \"\"\"\n",
        "    Returns avg and std inference latency (ms) over given runs.\n",
        "    \"\"\"\n",
        "\n",
        "    timer = Timer()\n",
        "    timings = np.zeros((repetitions, 1))\n",
        "\n",
        "    # warm-up\n",
        "    for _ in range(5):\n",
        "        _ = model(example_inputs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for rep in range(repetitions):\n",
        "            timer.start()\n",
        "            _ = model(example_inputs)\n",
        "            elapsed = timer.stop()\n",
        "            timings[rep] = elapsed\n",
        "\n",
        "    return np.mean(timings), np.std(timings)\n",
        "\n",
        "def estimate_latency_full(model, tag, skip_cpu):\n",
        "    \"\"\"\n",
        "    Prints model latency on GPU and (optionally) CPU.\n",
        "    \"\"\"\n",
        "\n",
        "    # estimate latency on CPU\n",
        "    if not skip_cpu:\n",
        "        example_input = torch.rand(128, 3, 32, 32).cpu()\n",
        "        model.cpu()\n",
        "        latency_mu, latency_std = estimate_latency(model, example_input)\n",
        "        print(f\"Latency ({tag}, on CPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")\n",
        "\n",
        "    # estimate latency on GPU\n",
        "    example_input = torch.rand(128, 3, 32, 32).cuda()\n",
        "    model.cuda()\n",
        "    latency_mu, latency_std = estimate_latency(model, example_input)\n",
        "    print(f\"Latency ({tag}, on GPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")\n",
        "\n",
        "def print_size_of_model(model, tag=\"\"):\n",
        "    \"\"\"\n",
        "    Prints model size (MB).\n",
        "    \"\"\"\n",
        "\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size_mb_full = os.path.getsize(\"temp.p\") / 1e6\n",
        "    print(f\"Size ({tag}): {size_mb_full:.2f} MB\")\n",
        "    os.remove(\"temp.p\")"
      ],
      "metadata": {
        "id": "oHCFHsCgIDCf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_to_quantize, train_dataloader, epochs=15, save_path=\"full_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3sa6vzxI24n",
        "outputId": "08422da5-2738-40d3-9a31-d9ac348bf3ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss=0.0003\n",
            "Accuracy (Epoch 1): 80.13%\n",
            "Epoch 2 Loss=0.0001\n",
            "Accuracy (Epoch 2): 80.17%\n",
            "Epoch 3 Loss=0.0003\n",
            "Accuracy (Epoch 3): 80.18%\n",
            "Epoch 4 Loss=0.0001\n",
            "Accuracy (Epoch 4): 80.22%\n",
            "Epoch 5 Loss=0.0001\n",
            "Accuracy (Epoch 5): 80.36%\n",
            "Epoch 6 Loss=0.0003\n",
            "Accuracy (Epoch 6): 80.37%\n",
            "Epoch 7 Loss=0.0001\n",
            "Accuracy (Epoch 7): 80.34%\n",
            "Epoch 8 Loss=0.0022\n",
            "Accuracy (Epoch 8): 80.15%\n",
            "Epoch 9 Loss=0.0002\n",
            "Accuracy (Epoch 9): 80.21%\n",
            "Epoch 10 Loss=0.0004\n",
            "Accuracy (Epoch 10): 80.37%\n",
            "Epoch 11 Loss=0.0001\n",
            "Accuracy (Epoch 11): 80.25%\n",
            "Epoch 12 Loss=0.0001\n",
            "Accuracy (Epoch 12): 80.33%\n",
            "Epoch 13 Loss=0.0001\n",
            "Accuracy (Epoch 13): 80.24%\n",
            "Epoch 14 Loss=0.0001\n",
            "Accuracy (Epoch 14): 80.34%\n",
            "Epoch 15 Loss=0.0001\n",
            "Accuracy (Epoch 15): 80.32%\n",
            "Training complete, model saved to --> full_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get full model size\n",
        "print_size_of_model(model_to_quantize, \"full\")\n",
        "\n",
        "# evaluate full accuracy\n",
        "accuracy_full = evaluate(model_to_quantize, 'full')\n",
        "\n",
        "# estimate full model latency\n",
        "estimate_latency_full(model_to_quantize, 'full', skip_cpu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKksydvIJXG9",
        "outputId": "bfa44c1b-724d-4250-cd5a-9b04e3870ad3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (full): 44.77 MB\n",
            "Accuracy (full): 80.31%\n",
            "Latency (full, on CPU): 1513.80 ± 193.57 ms\n",
            "Latency (full, on GPU): 27.41 ± 0.42 ms\n"
          ]
        }
      ]
    }
  ]
}