{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxGb0H1M45aPZcqaR+X9ke",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayanjabbaale/Model-Optimization-Using-Quantization/blob/main/QAT_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr1JQQOwukhN",
        "outputId": "ce3b5f5f-ddb5-47cc-e679-746050fa249d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version --> 2.9.0+cu126\n",
            "Working on --> cuda\n",
            "Skip CPU evaluations --> False\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "from packaging import version\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.quantization import quantize_dynamic\n",
        "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
        "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.ao.quantization\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*erase_node(.*) on an already erased node.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10.*\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*torch.ao.quantization is deprecated.*\", category=DeprecationWarning)\n",
        "\n",
        "print(f\"Using PyTorch version --> {torch.__version__}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Working on --> {device}')\n",
        "\n",
        "skip_cpu = False\n",
        "print(f'Skip CPU evaluations --> {skip_cpu}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), ((0.5,)))\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(\"./train\", train=True, transform=transform, download=True)\n",
        "test_data = datasets.CIFAR10(\"./eval\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              batch_size=128,\n",
        "                              shuffle=True,\n",
        "                              drop_last=True) # Added drop_last=True\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                              batch_size=128,\n",
        "                              shuffle=True,\n",
        "                              num_workers=2,\n",
        "                              drop_last=True)\n",
        "\n",
        "calibration_dataset = Subset(train_data, range(256))\n",
        "calibration_loader = DataLoader(calibration_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "6Zah8XSK8WG7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18_for_CIFAR10():\n",
        "  model = models.resnet18(weights=None, num_classes=10)\n",
        "  model.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "  model.maxpool = nn.Identity()\n",
        "\n",
        "  return model.to(device)\n",
        "\n",
        "model_to_quantize = resnet18_for_CIFAR10()"
      ],
      "metadata": {
        "id": "7PLU6Q3zAiVD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, epochs, lr=0.01, save_path=\"model.pth\", silent=False):\n",
        "  try:\n",
        "    model.train()\n",
        "  except NotImplementedError:\n",
        "    torch.ao.quantization.move_exported_model_to_train(model)\n",
        "\n",
        "  if os.path.exists(save_path):\n",
        "    if not silent:\n",
        "      print(f'Model already trained. Loading from --> {save_path}')\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    return\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for X, y in loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      logits = model(X)\n",
        "      loss = criterion(logits, y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if not silent:\n",
        "      print(f'Epoch {epoch+1} Loss={loss:.4f}')\n",
        "      evaluate(model, f'Epoch {epoch+1}')\n",
        "\n",
        "      try:\n",
        "        model.train()\n",
        "      except NotImplementedError:\n",
        "        torch.ao.quantization.move_exported_model_to_train(model)\n",
        "\n",
        "  if save_path:\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    if not silent:\n",
        "      print(f'Training complete, model saved to --> {save_path}')\n",
        "\n",
        "def evaluate(model, tag):\n",
        "  try:\n",
        "    model.eval()\n",
        "  except NotImplementedError:\n",
        "    model = torch.ao.quantization.move_exported_model_to_eval(model)\n",
        "\n",
        "  model.to(device)\n",
        "  correct = total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_dataloader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      preds = model(x).argmax(1)\n",
        "      correct += (preds == y).sum().item()\n",
        "      total += y.size(0)\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy ({tag}): {(accuracy*100):.2f}%')"
      ],
      "metadata": {
        "id": "0yrXjEaPBsCB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer:\n",
        "    \"\"\"\n",
        "    A simple timer utility for measuring elapsed time in milliseconds.\n",
        "\n",
        "    Supports both GPU and CPU timing:\n",
        "    - If CUDA is available, uses torch.cuda.Event for accurate GPU timing.\n",
        "    - Otherwise, falls back to wall-clock CPU timing via time.time().\n",
        "\n",
        "    Methods:\n",
        "        start(): Start the timer.\n",
        "        stop(): Stop the timer and return the elapsed time in milliseconds.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        if self.use_cuda:\n",
        "            self.starter = torch.cuda.Event(enable_timing=True)\n",
        "            self.ender = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    def start(self):\n",
        "        if self.use_cuda:\n",
        "            self.starter.record()\n",
        "        else:\n",
        "            self.start_time = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.use_cuda:\n",
        "            self.ender.record()\n",
        "            torch.cuda.synchronize()\n",
        "            return self.starter.elapsed_time(self.ender)  # ms\n",
        "        else:\n",
        "            return (time.time() - self.start_time) * 1000  # ms\n",
        "\n",
        "def estimate_latency(model, example_inputs, repetitions=50):\n",
        "    \"\"\"\n",
        "    Returns avg and std inference latency (ms) over given runs.\n",
        "    \"\"\"\n",
        "\n",
        "    timer = Timer()\n",
        "    timings = np.zeros((repetitions, 1))\n",
        "\n",
        "    # warm-up\n",
        "    for _ in range(5):\n",
        "        _ = model(example_inputs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for rep in range(repetitions):\n",
        "            timer.start()\n",
        "            _ = model(example_inputs)\n",
        "            elapsed = timer.stop()\n",
        "            timings[rep] = elapsed\n",
        "\n",
        "    return np.mean(timings), np.std(timings)\n",
        "\n",
        "def estimate_latency_full(model, tag, skip_cpu):\n",
        "    \"\"\"\n",
        "    Prints model latency on GPU and (optionally) CPU.\n",
        "    \"\"\"\n",
        "\n",
        "    # estimate latency on CPU\n",
        "    if not skip_cpu:\n",
        "        example_input = torch.rand(128, 3, 32, 32).cpu()\n",
        "        model.cpu()\n",
        "        latency_mu, latency_std = estimate_latency(model, example_input)\n",
        "        print(f\"Latency ({tag}, on CPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")\n",
        "\n",
        "    # estimate latency on GPU\n",
        "    example_input = torch.rand(128, 3, 32, 32).cuda()\n",
        "    model.cuda()\n",
        "    latency_mu, latency_std = estimate_latency(model, example_input)\n",
        "    print(f\"Latency ({tag}, on GPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")\n",
        "\n",
        "def print_size_of_model(model, tag=\"\"):\n",
        "    \"\"\"\n",
        "    Prints model size (MB).\n",
        "    \"\"\"\n",
        "\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size_mb_full = os.path.getsize(\"temp.p\") / 1e6\n",
        "    print(f\"Size ({tag}): {size_mb_full:.2f} MB\")\n",
        "    os.remove(\"temp.p\")"
      ],
      "metadata": {
        "id": "oHCFHsCgIDCf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_to_quantize, train_dataloader, epochs=15, save_path=\"full_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3sa6vzxI24n",
        "outputId": "ef26b4e9-8ace-415e-b585-3820bf2466ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss=1.2973\n",
            "Accuracy (Epoch 1): 56.58%\n",
            "Epoch 2 Loss=1.1122\n",
            "Accuracy (Epoch 2): 65.17%\n",
            "Epoch 3 Loss=0.7532\n",
            "Accuracy (Epoch 3): 66.94%\n",
            "Epoch 4 Loss=0.3445\n",
            "Accuracy (Epoch 4): 72.12%\n",
            "Epoch 5 Loss=0.4042\n",
            "Accuracy (Epoch 5): 74.23%\n",
            "Epoch 6 Loss=0.3669\n",
            "Accuracy (Epoch 6): 68.54%\n",
            "Epoch 7 Loss=0.0614\n",
            "Accuracy (Epoch 7): 73.40%\n",
            "Epoch 8 Loss=0.0407\n",
            "Accuracy (Epoch 8): 74.33%\n",
            "Epoch 9 Loss=0.0757\n",
            "Accuracy (Epoch 9): 76.02%\n",
            "Epoch 10 Loss=0.0533\n",
            "Accuracy (Epoch 10): 75.90%\n",
            "Epoch 11 Loss=0.0421\n",
            "Accuracy (Epoch 11): 77.11%\n",
            "Epoch 12 Loss=0.0352\n",
            "Accuracy (Epoch 12): 78.57%\n",
            "Epoch 13 Loss=0.0012\n",
            "Accuracy (Epoch 13): 79.42%\n",
            "Epoch 14 Loss=0.0004\n",
            "Accuracy (Epoch 14): 79.59%\n",
            "Epoch 15 Loss=0.0013\n",
            "Accuracy (Epoch 15): 79.91%\n",
            "Training complete, model saved to --> full_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get full model size\n",
        "print_size_of_model(model_to_quantize, \"full\")\n",
        "\n",
        "# evaluate full accuracy\n",
        "accuracy_full = evaluate(model_to_quantize, 'full')\n",
        "\n",
        "# estimate full model latency\n",
        "estimate_latency_full(model_to_quantize, 'full', skip_cpu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKksydvIJXG9",
        "outputId": "4377f3f3-9e12-4387-88d8-a27ad34c161c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (full): 44.77 MB\n",
            "Accuracy (full): 79.94%\n",
            "Latency (full, on CPU): 1659.39 ± 428.96 ms\n",
            "Latency (full, on GPU): 27.42 ± 1.05 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.ao.quantization.quantizer import x86_inductor_quantizer\n",
        "from torch.ao.quantization.quantize_pt2e import (\n",
        "    prepare_qat_pt2e,\n",
        "    convert_pt2e\n",
        ")\n",
        "\n",
        "import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq\n",
        "from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer\n",
        "\n",
        "example_inputs = (torch.randn((128, 3, 32, 32)).to(device), )\n",
        "\n",
        "if version.parse(torch.__version__) >= version.parse(\"2.5\"):\n",
        "  exported_model = torch.export.export(model_to_quantize, example_inputs).module()\n",
        "else:\n",
        "  from torch._export import capture_pre_autograd_graph\n",
        "  exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs)\n",
        "\n",
        "quantizer = X86InductorQuantizer()\n",
        "quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())\n",
        "\n",
        "prepared_model = prepare_qat_pt2e(exported_model, quantizer)\n",
        "\n",
        "train(prepared_model, train_dataloader, epochs=3, save_path=\"x86_quant_model.pth\")\n",
        "\n",
        "quantized_model = convert_pt2e(prepared_model)\n",
        "\n",
        "if version.parse(torch.__version__) >= version.parse(\"2.5\"):\n",
        "  quantized_model = torch.export.export(quantized_model, example_inputs).module()\n",
        "else:\n",
        "  from torch._export import capture_pre_autograd_graph\n",
        "  quantized_model = capture_pre_autograd_graph(quantized_model, example_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6N53XT8tYQw",
        "outputId": "31992858-e537-4f9f-fa4d-90929d093b57"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss=0.0001\n",
            "Accuracy (Epoch 1): 80.20%\n",
            "Epoch 2 Loss=0.0001\n",
            "Accuracy (Epoch 2): 80.11%\n",
            "Epoch 3 Loss=0.0002\n",
            "Accuracy (Epoch 3): 80.18%\n",
            "Training complete, model saved to --> x86_quant_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get quantized model size\n",
        "print_size_of_model(quantized_model, \"quantized\")\n",
        "\n",
        "# evaluate quantized accuracy\n",
        "accuracy_full = evaluate(quantized_model, 'quantized')\n",
        "\n",
        "# estimate quantized model latency\n",
        "estimate_latency_full(quantized_model, 'quantized', skip_cpu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVXpmJE31-up",
        "outputId": "3e6632da-ecba-485c-aff4-7c0b05c8967e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (quantized): 44.76 MB\n",
            "Accuracy (quantized): 80.17%\n",
            "Latency (quantized, on CPU): 1613.42 ± 198.74 ms\n",
            "Latency (quantized, on GPU): 30.61 ± 1.50 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# enable the use of the C++ wrapper for TorchInductor which reduces Python overhead\n",
        "import torch._inductor.config as config\n",
        "config.cpp_wrapper = True\n",
        "\n",
        "# compiles quantized model to generate optimized model\n",
        "with torch.no_grad():\n",
        "    optimized_model = torch.compile(quantized_model)\n",
        "\n",
        "# get optimized model size\n",
        "print_size_of_model(optimized_model, \"optimized\")\n",
        "\n",
        "# evaluate optimized accuracy\n",
        "accuracy_full = evaluate(optimized_model, 'optimized')\n",
        "\n",
        "# estimate optimized model latency\n",
        "estimate_latency_full(optimized_model, 'optimized', skip_cpu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38q7x1Wj-W_Y",
        "outputId": "42a52fc2-e001-4585-def4-d1cff22379bc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (optimized): 44.76 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "W0205 09:09:45.453000 595 torch/_inductor/utils.py:1558] [930/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (optimized): 80.20%\n",
            "Latency (optimized, on CPU): 1497.42 ± 1032.84 ms\n",
            "Latency (optimized, on GPU): 52.14 ± 0.50 ms\n"
          ]
        }
      ]
    }
  ]
}